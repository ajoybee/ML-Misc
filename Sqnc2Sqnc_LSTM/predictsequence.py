# -*- coding: utf-8 -*-
"""PredictSequence.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kQ13MeANzI5ZqAXzQAwzhrg5OBUpv5OS
"""
import os
import math

def is_interactive():
    import __main__ as main
    return not hasattr(main, '__file__')

nn_model = ""
excel_file = ""
if ( is_interactive() ):
	print ( is_interactive() )

#	!pip install xlrd
	# upload test excel file
	from google.colab import files
	uploaded = files.upload()
	for xl in uploaded.keys():
	  print('Uploaded Test file "{name}" with length {length} bytes'.format(
			 name=xl, length=len(uploaded[xl])))
	  excel_file = xl
	  break

	# upload neural network model file.
	# It'll be present in <Downloads> dir with name "nn_model.h5"	  
	uploaded = files.upload()
	for mdl in uploaded.keys():
	  print('Uploaded Mddel file "{name}" with length {length} bytes'.format(
			 name=mdl, length=len(uploaded[mdl])))
	  nn_model = mdl
	  break
else:
	os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
	import sys
	if ( len(sys.argv) >= 3 ) :
		excel_file = sys.argv[1]
		nn_model   = sys.argv[2]
	else:
		print("\nUsage: ", sys.argv[0], "excel_file ai_model")
		sys.exit("\tTry again.")

if ( excel_file == "" ):
	raise IOError("Excel file not provided.")
if ( nn_model == "" ):
	raise IOError("Model file not provided.")
	
import numpy as np
import pandas as pd
import keras

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.models import load_model
from sklearn.preprocessing import MinMaxScaler

print("Keras version", keras.__version__)
print("Numpy version", np.__version__)
 
df_xl = pd.read_excel(excel_file)
model = load_model(nn_model)

df      = df_xl.transpose()
if ( is_interactive() ):
	display(df[:5])

batch_size   = 1
look_back    = 2
n_features   = 7

# convert an array of values into a dataset matrix
def create_dataset(dataset, look_back):
	dataX, dataY = [], []
	for i in range(len(dataset)-look_back-1):
		a = dataset[i:(i+look_back), :]
		dataX.append(a)
		dataY.append(dataset[i + look_back, :])
	return np.array(dataX), np.array(dataY)


# normalize dataset
dataset = df.values
dataset = dataset.astype('float32')
# normalize the dataset
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)

if ( n_features != dataset.shape[1] ):
  raise ValueError("Excel file format does not match with trained model.")

test = dataset[35:,:]

# reshape into X=t and Y=t+1

testX, testY = create_dataset(test, look_back)
testPredict = model.predict(testX, batch_size=batch_size)

normalizedTestPrediction =  scaler.inverse_transform(testPredict)
#normalizedTest           =  scaler.inverse_transform(test[3:])

print("Prediction for last 6 columns:\n",
      normalizedTestPrediction[:,1:].astype(dtype=np.int32))